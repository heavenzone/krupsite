---
title: 用R语言对《射雕英雄传》分词分析
author: Heaven Zone
date: '2017-12-20'
slug: characters-relationship-in-novel-shediao
categories:
  - 文本挖掘
tags:
  - 文本挖掘
  - R语言
  - 射雕英雄传
description: '射雕英雄传人物关系图，用R语言对《射雕英雄传》分词分析'
keywords: "R语言,文本挖掘,射雕英雄传"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


<script src="/js/wordcloud2-all.js"></script>
<script src="/js/hover.js"></script>
<script src="/js/wordcloud2.js"></script>


## 简介

目标：对《射雕英雄传》进行分词分析，画出人物云图，人物关系图。

人物关系算法参考[这里](https://zhuanlan.zhihu.com/p/26104216)。



## 软件包准备

### 安装软件包

主要用到下面几个软件包：

- jiebaR
- cidian
- ggnetwork
- igraph

首先应该把**devtools**安装好了，也就是一条命令`install.packages("devtools")`。

然后就可以输入下面几条命令安装本文档所需的软件包的最新版了：

```r
library(devtools)
install_github("qinwf/jiebaRD")
install_github("qinwf/jiebaR")
install.packages("stringi")
install.packages("pbapply")
install.packages("Rcpp")
install.packages("RcppProgress")
install_github("qinwf/cidian")
install.packages("text2vec")
install.packages("tm")
install.packages("intergraph") # 连接ggnetwork和igraph
install.packages("ggnetwork") # 画关系图
install.packages("igraph") # graph.data.frame
```
### 载入软件包

```{r}
library(cidian)
library(jiebaR)
library(data.table)
library(wordcloud2)
library(tm)


```


## 词库准备

### 下载搜狗细胞词库

首先到这里[细胞词库下载地址](https://pinyin.sogou.com/dict/search/search_list/%BD%F0%D3%B9/normal)下载搜狗的细胞词库。

一共下载了下面6个词库。

```{r}
dir(path = "../../data/20171220-shediao/", pattern = "scel$")
```

### 转换词库

下面将搜狗的细胞词库转换成jiebaR的用户词库:

```{r}
# 如果已经存在.scel.txt文件，把他们删除
tmp <- list.files(path = "../../data/20171220-shediao/",pattern = ".scel.txt$",full.names = T) 
if(length(tmp) >= 1) {
  lapply(tmp,function(x){
    file.remove(x)
  }) %>% invisible()
}

# 重新生成.scel.txt，用户词典
tmp <- list.files(path = "../../data/20171220-shediao/",pattern = ".scel$",full.names = T) 
scelFiles <- paste0(getwd(), "/", tmp)
lapply(scelFiles,function(x){
  decode_scel(scel = x, 
              cpp = TRUE,
              output = str_c(x,".txt"))
  }) %>% invisible()

# 查看生成的文件
list.files(path = "../../data/20171220-shediao/",pattern = ".scel.txt$",full.names = T) 
```

再把生成的这些jiebaR词库合并成一个文件：

```{r}
dictList <- list.files(path = "../../data/20171220-shediao/",
                       pattern = "scel.txt$",
                       full.names = T)  
# 加入一些微调词语，例如上面词库里面没有的词语：蓉儿
dictList[length(dictList) + 1] <- "../../data/20171220-shediao/user.dict"
dictToBeCombine <- lapply(dictList,
                          function(x)
                            fread(x,encoding = "UTF-8", header = FALSE))

# 合并词库
jinYongDict <- rbindlist(dictToBeCombine) 

# 删除重复词条
jinYongDictUni <- unique(jinYongDict, by = names(jinYongDict))  

# 写入文件保存
write.table(jinYongDictUni,file = "../../data/20171220-shediao/jinYong.utf8.dict",
            quote = F,
            row.names = F,
            col.names = F,
            fileEncoding = "UTF-8")
```



## 分词处理

```{r}
wk = worker(user = "../../data/20171220-shediao/jinYong.utf8.dict")
novelText <- read_lines("../../data/20171220-shediao/shediao.utf8.txt")
#segment("../../data/20171220-shediao/shediao.utf8.txt", wk)
wordFreq <- wk[novelText]
freq(wordFreq) %>% arrange(-freq) %>% head

# 读入stop_words
stopWords <- readLines("../../data/20171220-shediao/mystopword.txt", encoding = 'UTF-8')
# 去除stop_words
target_words = wordFreq[wordFreq%in%stopWords==FALSE]
freqTable <- freq(target_words) %>% arrange(-freq)
# 删除一些高词频无意义的词语
removeWords <- c("道", "听", "说", "中", "想", "之中", "罢", "走", "做")
freqTable <- freqTable %>% dplyr::filter(!(char %in% removeWords))

freqTable %>% head
```

## 词云输出


```{r}
library(widgetframe)
ts <- wordcloud2(freqTable, color = "random-light", backgroundColor = 'black')
frameWidget(ts)
```


### 人物云图

关于**金庸人名.scel**这个词库，里面缺少**成吉思汗**这个重要人物，增加了**完颜康**等名字，还有**欧阳锋**这个名字错打成了**欧阳峰**，手工编辑了一个`shediao.renwu.user.dict`文件，然后合并到`金庸人名.scel.txt`，然后再读入。

```{r}
# 加入了一些人名，例如蓉儿、完颜康、傻姑
file.append( "../../data/20171220-shediao/金庸人名.scel.txt",
             "../../data/20171220-shediao/shediao.renwu.user.dict")
wkRenwu <-  worker(user = "../../data/20171220-shediao/金庸人名.scel.txt")
renwuTmp <- wkRenwu[novelText]
freq(renwuTmp) %>% arrange(-freq) %>% head

# 读入stop_words
stopWords <- readLines("../../data/20171220-shediao/mystopword.txt", encoding = 'UTF-8')
# 去除stop_words
targetRenwu <-  renwuTmp[renwuTmp %in% stopWords == FALSE]
freqRenwu <- freq(targetRenwu) %>% arrange(-freq)
# 删除一些高词频无意义的词语
removeWords <- c("道", "听", "说", "中", "想", "之中", "罢", "走", "做")
freqRenwu <- freqRenwu %>% dplyr::filter(!(char %in% removeWords))

# 帅选人名
renwuWords <- fread(file = "../../data/20171220-shediao/金庸人名.scel.txt", 
                    select = 1,
                    header = FALSE,
                    encoding = "UTF-8")
freqRenwu2 = freqRenwu[freqRenwu$char%in%renwuWords[["V1"]],]
freqRenwu2 %>% dim
wordcloud2(freqRenwu2, color = "random-light", backgroundColor = 'grey25')
```


### 人物关系图

这里人物关系的算法的核心是，同一段落同时出现的名字之间的权重加1。

```{r}
novelText2 <- novelText
renwuAll <- freqRenwu2
novelText2 <- novelText2[novelText2 != ""]

n <- length(novelText2)
paragraphs <- vector('list',n)
wk.engine <-  worker(user = "../../data/20171220-shediao/金庸人名.scel.txt")

for(i in 1:length(paragraphs)) paragraphs[[i]] = wk.engine[novelText2[i]][which(wk.engine[novelText2[i]] %in% renwuAll$char)]

weight.data <- data.frame(t(combn(renwuAll$char,2)))
names(weight.data)=c('name1','name2')
weight.data <- weight.data %>% unite(col = "weightName",
                      1:2, 
                      sep = "-@-", 
                      remove = FALSE)

weight.data$weight <- rep(0, dim(weight.data)[1])
for(i in 1:length(paragraphs)) {
  test <- as.data.frame(table(paste(expand.grid(paragraphs[[i]],paragraphs[[i]])$Var1,expand.grid(paragraphs[[i]],paragraphs[[i]])$Var2,sep = '-@-')), stringsAsFactors = FALSE)
  test$Freq = test$Freq/max(test$Freq)
  id1 <- which(test$Var1 %in% weight.data$weightName)
  id2 <- which(weight.data$weightName %in% test$Var1)
  weight.data$weight[id2] <- weight.data$weight[id2] + test$Freq[id1]
}

weight.data %>% arrange(-weight) %>% head(20)

# 绘制关系图
paragraphs.chars <- paragraphs %>% unlist()
taltext <- as.data.frame(table(wk.engine[paragraphs.chars][which(wk.engine[paragraphs.chars] %in% renwuAll$char)]))

ind3 <- rep(0, dim(taltext)[1])
for(i in 1:dim(taltext)[1]) {
  ind3[i] = taltext$Freq[which(taltext$Var1[i] == renwuAll$char)]
}


# 画出人物关系图
library(igraph)
library(ggnetwork)
guanxi.data <- weight.data[which(weight.data$weight != 0), ]
g1=guanxi.data %>% 
  select(-weightName) %>% 
  arrange(-weight) %>% 
  slice(1:30) %>%
  graph.data.frame(directed = F)

ggplot2::fortify(g1) %>%
  ggplot(aes(x, y, xend = xend, yend = yend)) +
  geom_edges(linetype = 2, color = "grey50",curvature = 0.1) +
  geom_nodes(aes(color =  vertex.names, size =  weight)) +
  geom_nodelabel_repel(aes(color = vertex.names, label = vertex.names),
                       fontface = "bold", box.padding = unit(1, "lines")) +
  theme(legend.position='none',
        axis.text = element_blank(),
        axis.title = element_blank(),
  panel.background = element_rect(fill = "grey25"),
  panel.grid = element_blank()
  )
```





## 参考资料
- [jiebaR官方文档](http://qinwenfeng.com/jiebaR/)
-
 [细胞词库下载地址](https://pinyin.sogou.com/dict/search/search_list/%BD%F0%D3%B9/normal)
- [R语言中文分词包jiebaR
](http://blog.fens.me/r-word-jiebar/)
- <http://blog.csdn.net/nicolelovesmath/article/details/73835499>
- <https://zhuanlan.zhihu.com/p/26104216>



