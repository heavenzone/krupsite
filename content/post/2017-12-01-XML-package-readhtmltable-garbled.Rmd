---
title: 关于XML包readHTMLTable乱码
author: Heaven Zone
date: '2017-12-01'
slug: XML-package-readhtmltable-garbled
categories:
  - R
tags:
  - R
  - XML
  - readHTMLTable
description: 'readHTMLTable乱码怎么解决呢？'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require(xml2)
require(XML)
require(RCurl)
require(rvest)
```


## 目的

现在要爬p2peye的页面http://www.p2peye.com/shuju/ptsj/ ，R的XML包的readHTMLTable函数提供了非常方便的读取网页表格的方法。


## 用readHTMLTable

说干就干，马上调出readHTMLTable。

```{r message=FALSE, warning=FALSE}
library(XML)
url = "http://www.p2peye.com/shuju/ptsj/"
p2ptable = readHTMLTable(url, header = FALSE, stringsAsFactors = F)
head(p2ptable[["platdata"]][,1:3])
```

表格读取到了，但是出现乱码了。


## 用getURL

在这里直接导入四个跟爬虫有关的包XML、xml2、RCurl和rvest，因为我真的是记不住那些函数究竟是哪个包里面的了。

搜了很多网站去查询关于抓网页出现乱码的问题，国内国外，很多情况都喜欢在readHTMLTable前用getURL弄一下设置一下encoding，ok，我们试一下吧。

我们查看了该网页的源代码是有一个`charset=gbk`的设置，因此我们把encoding设置成了gbk：

```{r message=FALSE, warning=FALSE}
require(xml2)
require(XML)
require(RCurl)
require(rvest)
url = "http://www.p2peye.com/shuju/ptsj/"
p2ptable <- getURL(url, .encoding = "UTF-8")
p2ptable = readHTMLTable(p2ptable, header = FALSE, stringsAsFactors = F)
head(p2ptable[["platdata"]][,1:3])
```

还是出现乱码。

尝试把gbk更改为UTF-8也是一样的乱码。

国外一些有人也喜欢用readLines代替getURL，所以也尝试了将上面这行代码：

```r
p2ptable <- getURL(url, .encoding = "UTF-8")
```

改为

```r
p2ptable = readLines(url, encoding = "UTF-8")
```

结果还是乱码。


## 一些测试

### 测试htmlParse

```r
htmlParse(url, encoding = "gbk")
htmlParse(url, encoding = "UTF-8")
```

上面两个代码得到的结果都是乱码。


### 测试getURL

```r
getURL(url, .encoding = "gbk")
```

用getURL，并设置好参数`.encoding = "gbk"`，得到的**没有乱码**。


```r
getURL(url, .encoding = "UTF-8")
```

如果`.encoding`设置成`UTF-8`得到的结果乱码。

说明getURL可以通过设置encoding来避免出现乱码，但是前面的测试结果显示，就算getURL可以得到正常的结果，但是通过readHTMLTable读取getURL的结果仍然是出现乱码。


## linux下的readHTMLTable没有乱码问题

完全一样的代码，我在linux系统manjaro-i3（locale当然是UTF-8的）上用，测试下面这段代码，就没有出现乱码。

```{r message=FALSE, warning=FALSE}
library(XML)
url = "http://www.p2peye.com/shuju/ptsj/"
p2ptable = readHTMLTable(url, header = FALSE, stringsAsFactors = F)
head(p2ptable[["platdata"]][,1:3])
```

难道就如[这里](http://www.dataguru.cn/thread-567670-1-1.html)所说的？

>你这个是在windows系统下抓取的吧？XML在windows系统下不能很好的处理中文

或者如[这里](http://blog.csdn.net/BieberSen/article/details/50789702)所说？

>如果HTML中本身已经指定了编码（此处就是，但是有2个charset。。。前一个是GB2312，后一个是UTF-8），那么就会 强制 使用HTML中内部指定的编码而忽略调用者（此处我们的代码所传入的GBK）
>
>所以即使调用者指定了正确的HTML的编码，结果也还是使用HTML内部自己所指定的错误的编码（此处应该就是用了第二个charset，即UTF-8来解析的）从而导致乱码的。
>
>差不多算是一个bug吧 


## 小结

简单做一个小结，我们上面一共用到了readHTMLTable、getURL、htmlParse这三个函数，看看他们属于哪个包？

- XML::readHTMLTable()
- XML::htmlParse
- RCurl::getURL()

难道真的如前面两个网站所说的？XML在windows系统下不能很好的处理中文？或者这是一个bug？

不过我们还是有另外一个包可以处理。

## 最终解决乱码的办法

不过最终我们还是有办法处理这个问题，也是能够像readHTMLTable一样的方便直接读取表格数据。

这次我们用到了**rvest**的html_table函数和**xml2**的read_html函数。


```{r}
library("rvest")
webpage <- read_html(url, fill = TRUE)
webpage.table <- html_table(webpage)
head(webpage.table[[2]][2:5])
```

当然我们也可以用到**rvest**的**html_nodes**会更方便，如果该网页有多个table表格，那么就可以直接用css来定位要获取的表格，下面练习了一下`%>%`的方式来写代码。

```{r}
webpage.table2 <- url %>%
  read_html() %>%
  html_nodes(css ='#platdata') %>%
  html_table() %>% .[[1]]
webpage.table2 %>% .[1:4] %>% head
```

结果就是我们想要的，中国人能读的中文了。


## 参考资料

- [Scraping html tables into R data frames using the XML package](https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package)
- [Using rvest to Scrape an HTML Table](https://www.r-bloggers.com/using-rvest-to-scrape-an-html-table/)
- [如何处理readHTMLTable函数抓取乱码数据](http://www.dataguru.cn/thread-567670-1-1.html)
- [使用RCurl爬虫爬取网页内容htmlParse解析时出现乱码](http://blog.csdn.net/BieberSen/article/details/50789702)
- [【记录】尝试用R语言去抓取网页和提取信息](https://www.crifan.com/try_use_r_language_do_web_crawl_and_extract_info/)