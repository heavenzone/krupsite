---
title: 抓取basketball-conference的表格数据
author: Heaven Zone
date: '2017-12-03'
slug: r-scrape-basketball-reference-data-table
categories:
  - R
tags:
  - R
  - NBA
description: 'R语言用正则表达式处理html文件获取表格数据'
keywords: 
  - R语言2
  - 正则表达式
---

## 目的

抓取网页【<https://www.basketball-reference.com/leagues/NBA_2017.html>】中**Team Per Game Stats**表格数据。


## 问题

其实<http://www.stat-nba.com>已经有NBA的各种历史统计数据，而且可以直接下载到电子表格，但是还是认为英文版的处理数据会更方便一点，于是找到<https://www.basketball-reference.com>网站，网站打开也非常的简洁，本以为可以很方便的就抓取到数据了，可是打开页面【<https://www.basketball-reference.com/leagues/NBA_2017.html>】想抓取**Team Per Game Stats**表格数据的时候，发现并没有抓到这个表格和后面所有表格的数据，只能检测到4个表格，后来查看网页源代码发现这个表格和后面所有表格的源代码都是绿色的，都是用`<!-- ... -->`注释掉了，估计是通过js处理才把数据显示出来的。

## 处理思路

- 获取网页源代码
- 通过正则表达式把该注释符号去掉
- 读取处理后的网页源代码
- 获取表格数据

自己对正则表达式不怎么熟悉，所以写出来这个正则表达式把自己也吓了一跳，搜索资料的时候主要是搜索关于**正则表达式 包含指定字符串**和**正则表达式 不包含指定字符串**，我还真搞不懂正则表达式的专业术语是什么东东，例如什么**前瞻后顾**，什么**反向引用**，不知所云，好在通过不断的实践，写出了下面这么长的一个正则表达式：

```
html.file <- gsub(pattern = "<!--((?:(?!(<!--)|(-->))[\\w\\W\u4e00-\u9fa5\r\n])*?<table(?:(?!(<!--)|(-->))[\\w\\W\u4e00-\u9fa5\r\n])*?)-->","\\1",x = html.file, perl = TRUE)
```

## 代码实现

习惯性载入这几个爬虫包，然后在网上搜索的时候不小心了解到了htmltab包，不过这个包似乎不能很好处理UTF-8的字符，不过我们这里要处理的网页基本是全英文的，为什么说是基本呢？因为我们要获取的表格数据确实隐含了一些UTF-8字符（不知道这里说是*UTF-8字符*是否合适，或许叫*unicode字符*？），没关系，htmltab在处理表格数据的时候确实是比较方便的，因为它自动帮我们预处理了很多东西，我们在这里会尝试试用一下这个包。

### 载入包：

```{r setup,	message = FALSE,	warning = FALSE, echo = TRUE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	echo = TRUE
)
library(XML)
library(xml2)
library(RCurl)
library(rvest)
library(stringr)
library(tidyverse)
library(htmltab)
```

### 方法一：

```{r}
url <- "https://www.basketball-reference.com/leagues/NBA_2017.html"
# 读入网页数据
webpage <- read_html(url)

# 把一些无关紧要的源代码去除掉，后面重新读入的时候更快
html.tmp <- gsub(pattern = "<meta.*?>|<link.*?>|<img.*?>|<a.*?>|</a>|\r\n|<script.*?</script>|<style.*?</style>","",x = webpage)

# 这里是重点，把将我们要读取的表格外层的注释符号去除掉
html.tmp <- gsub(pattern = "<!--((?:(?!(<!--)|(-->))[\\w\\W\u4e00-\u9fa5\r\n])*?<table(?:(?!(<!--)|(-->))[\\w\\W\u4e00-\u9fa5\r\n])*?)-->","\\1",x = html.tmp, perl = TRUE)

# 读取ID为team-stats-per_game的表格数据
webpage.table <- htmltab(html.tmp, which ="//table[@id='team-stats-per_game']" )

# 输出表格数据
# 为了不影响页面排班，输出结果只输出10列10行
knitr::kable(webpage.table %>% select(1:10) %>% slice(1:10))
```




### 方法二：

其实我是先用了这个办法的，把网页直接下载下来保存成一个临时文件，原本想通过处理文本文件的方法把注释处理掉，然后再重新读入的，不过后来发现这个过程有点像绕了一个大圈，还好最后测试了一下还是可以像方法一那样直接读入变量进行处理，不过为了记录方法二这个过程，这里还是把这个过程记录下来。

```{r}
url <- "https://www.basketball-reference.com/leagues/NBA_2017.html"

# 下载到临时文件
download.file(url,"../../data/tmp.html", quiet = TRUE)

# 通过read_file读入变量
html.file <- read_file("../../data/tmp.html")

# 去除html无用的代码
html.file <- gsub(pattern = "<meta.*?>|<link.*?>|<img.*?>|<a.*?>|</a>|\r\n|<script.*?</script>|<style.*?</style>","",x = html.file)

# 去掉表格数据外层的注释
html.file <- gsub(pattern = "<!--((?:(?!(<!--)|(-->))[\\w\\W\u4e00-\u9fa5\r\n])*?<table(?:(?!(<!--)|(-->))[\\w\\W\u4e00-\u9fa5\r\n])*?)-->","\\1",x = html.file, perl = TRUE)

# 重新写入html文件
write_file(html.file,"../../data/tmp.html")

# 通过read_html读入处理过的html文件
webpage.new2 <- read_html("../../data/tmp.html")

# 获取表格数据
webpage.table2 <- html_nodes(webpage.new2, css = "#team-stats-per_game")
webpage.table2 <- html_table(webpage.table2[[1]])

# 删除临时文件
file.remove("../../data/tmp.html")
# 输出表格
# 为了不影响页面排班，输出结果只输出10列10行
knitr::kable(webpage.table2 %>% select(1:10) %>% slice(1:10))
```


### 小结

我们可以看到两种方法都已经成功把表格数据抓出来了，剩下的就是一些数据清洗工作了，例如把Team那一列的城市名称和队名分离出来，去除掉一些*****号，把一些列转换成数值型等等。

由于为了有更美观的输出结果，上面我都用了`knitr::kable`来输出表格数据，下面我们再用`str`命令来查看一下`htmltab::htmltab`和`rvest::html_table`的输出结果有什么区别，看看哪个更好。

```{r}
str(webpage.table)
str(webpage.table2)
```

可以看到，用**htmltab**返回的是一个data.frame，储存为数字的列的类型是字符串，而用**html_table**返回的也是一个data.frame，不过为数字的列自动已经帮我们转化成数值型，让我们省去了再去转化的过程。

看来还是用**rvest::html_table**更好一点哦。


## 参考资料

- [正则表达式 不包含指定字符串](http://blog.csdn.net/maqingli20/article/details/7317925)
- [正则基础之——反向引用](http://www.cnblogs.com/-ShiL/archive/2012/04/06/Star201204061009.html)




